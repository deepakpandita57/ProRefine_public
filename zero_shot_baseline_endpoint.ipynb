{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import textgrad as tg\n",
    "from textgrad.tasks import load_task\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLAMA 3.1 8B\n",
    "endpoint_url = \"llama_3.1_8B_endpoint_url\"\n",
    "endpoint_model_name = \"meta/llama-3.1-8b-instruct\"\n",
    "\n",
    "\n",
    "# LLAMA 3.1 70B\n",
    "# endpoint_url = \"llama_3.1_70B_endpoint_url\"\n",
    "# endpoint_model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "\n",
    "headers = {\"insert headers here\"}\n",
    "\n",
    "\n",
    "def get_response_from_endpoint(endpoint_url, endpoint_model_name, headers, messages):\n",
    "    json_data = {\n",
    "        \"model\": endpoint_model_name,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    response = requests.post(endpoint_url, headers=headers, json=json_data)\n",
    "    response_json = json.loads(response.text)\n",
    "    response_text = (\n",
    "        response_json.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n",
    "    )\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platformdirs\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "from textgrad.engine.base import EngineLM, CachedEngine\n",
    "\n",
    "\n",
    "class LLamaModel(EngineLM, CachedEngine):\n",
    "    DEFAULT_SYSTEM_PROMPT = \"You are a helpful, creative, and smart assistant.\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint_url,\n",
    "        headers,\n",
    "        endpoint_model_name=\"meta/llama-3.1-70b-instruct\",\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param endpoint_model_name:\n",
    "        :param system_prompt:\n",
    "        \"\"\"\n",
    "        root = platformdirs.user_cache_dir(\"textgrad\")\n",
    "        cache_path = os.path.join(root, f\"cache_llama_{endpoint_model_name}.db\")\n",
    "        super().__init__(cache_path=cache_path)\n",
    "\n",
    "        self.system_prompt = system_prompt\n",
    "        if os.getenv(\"API_KEY\") is None:\n",
    "            raise ValueError(\n",
    "                \"Please set the API_KEY environment variable if you'd like to use the LLama model.\"\n",
    "            )\n",
    "\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.headers = headers\n",
    "        self.endpoint_model_name = endpoint_model_name\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt,\n",
    "        system_prompt=None,\n",
    "    ):\n",
    "\n",
    "        sys_prompt_arg = system_prompt if system_prompt else self.system_prompt\n",
    "\n",
    "        cache_or_none = self._check_cache(sys_prompt_arg + prompt)\n",
    "        if cache_or_none is not None:\n",
    "            return cache_or_none\n",
    "\n",
    "        json_data = {\n",
    "            \"model\": self.endpoint_model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": sys_prompt_arg},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"max_tokens\": 2048,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        response = requests.post(\n",
    "            self.endpoint_url, headers=self.headers, json=json_data\n",
    "        )\n",
    "        response_json = json.loads(response.text)\n",
    "        response = (\n",
    "            response_json.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\")\n",
    "        )\n",
    "        self._save_cache(sys_prompt_arg + prompt, response)\n",
    "        return response\n",
    "\n",
    "    @retry(wait=wait_random_exponential(min=1, max=5), stop=stop_after_attempt(5))\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        return self.generate(prompt, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "\n",
    "* You need to have an API key to run this. This should be set as an environment variable as API_KEY.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test Set Lengths:  50 100 100\n",
      "You will answer a reasoning question. Think step by step. The last line of your response should be of the following format: 'Answer: $VALUE' where VALUE is a numerical value.\n"
     ]
    }
   ],
   "source": [
    "eval_endpoint_url = \"eval_endpoint_url\"\n",
    "eval_endpoint_model_name = \"meta/llama-3.1-70b-instruct\"\n",
    "llm_api_eval = LLamaModel(\n",
    "    endpoint_url=eval_endpoint_url,\n",
    "    headers=headers,\n",
    "    endpoint_model_name=eval_endpoint_model_name,\n",
    ")\n",
    "\n",
    "# llm_api_eval = tg.get_engine(engine_name=\"\")\n",
    "\n",
    "# train_set, val_set, test_set, eval_fn = load_task(\n",
    "#     \"BBH_object_counting\", evaluation_api=llm_api_eval\n",
    "# )\n",
    "\n",
    "train_set, val_set, test_set, eval_fn = load_task(\n",
    "    \"BBH_word_sorting\", evaluation_api=llm_api_eval\n",
    ")\n",
    "\n",
    "# train_set, val_set, test_set, eval_fn = load_task(\n",
    "#     \"GSM8K_DSPy\", evaluation_api=llm_api_eval\n",
    "# )\n",
    "\n",
    "print(\"Train/Val/Test Set Lengths: \", len(train_set), len(val_set), len(test_set))\n",
    "STARTING_SYSTEM_PROMPT = train_set.get_task_description()\n",
    "prompt = STARTING_SYSTEM_PROMPT\n",
    "print(STARTING_SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sort the following words alphabetically: List: hat core sonnet discreet',\n",
       " 'core discreet hat sonnet')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = val_set[0][0]\n",
    "label_str = val_set[0][1]\n",
    "input_str, label_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prediction(text, model_name):\n",
    "    if \"llama\" in model_name:\n",
    "        splitted_text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "        if len(splitted_text) > 1:\n",
    "            label = splitted_text[1].split(\"<|eot_id|>\")[0].strip()\n",
    "        else:\n",
    "            label = text\n",
    "    else:\n",
    "        label = text\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(model_name):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        offload_buffers=True,\n",
    "    )\n",
    "\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return (tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_LLM(\n",
    "    messages,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    model_name,\n",
    "    device=\"cuda\",\n",
    "    max_new_tokens=500,\n",
    "):\n",
    "    # messages = feedback_tokenizer.apply_chat_template(\n",
    "    #     [\n",
    "    #         {\"role\": \"system\", \"content\": prompt},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Input: {input_str}\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Output: {output_str}\"},\n",
    "    #     ],\n",
    "    #     tokenize=False,\n",
    "    # )\n",
    "    encodeds = tokenizer.encode(\n",
    "        messages, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    attention_mask = (encodeds != tokenizer.pad_token_id).long()\n",
    "\n",
    "    model_inputs = encodeds.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,\n",
    "        # top_k=50,\n",
    "        # top_p=0.95,\n",
    "        top_p=None,\n",
    "        temperature=None,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    output = extract_prediction(decoded[0], model_name)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# # model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# tokenizer_llm_call, model_llm_call = prepare_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(input_str, system_prompt, tokenizer=None):\n",
    "    if tokenizer is not None:\n",
    "        text_input = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Input: {input_str}\"},\n",
    "            ],\n",
    "            tokenize=False,\n",
    "        )\n",
    "    else:\n",
    "        text_input = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Input: {input_str}\"},\n",
    "        ]\n",
    "    return text_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_request = prepare_input(input_str, prompt, tokenizer_llm_call)\n",
    "# output = call_LLM(text_request, model_llm_call, tokenizer_llm_call, model_name)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sort the words alphabetically, I will follow these steps:\n",
      "\n",
      "1. Compare the first letter of each word:\n",
      "- hat: h\n",
      "- core: c\n",
      "- discreet: d\n",
      "- sonnet: s\n",
      "\n",
      "From left to right, the order is: c, d, h, s\n",
      "\n",
      "2. If two or more words have the same first letter, compare the second letter:\n",
      "- core: o\n",
      "- discreet: i\n",
      "- hat: a\n",
      "- sonnet: o\n",
      "\n",
      "From left to right, the order is still: c, d, h, s, but with i and a as additional comparisons.\n",
      "\n",
      "3. If the second letters are also the same, compare the third letter:\n",
      "- discreet: c\n",
      "- hat: t\n",
      "\n",
      "So, discreet comes before hat.\n",
      "\n",
      "My comparison results in: c, d, d (discreet is before discreet because of its spelling), h, s\n",
      "\n",
      "4. Now considering the unique combination, the spelling of discreet and its proper placement: c, d (discreet), discrete (which is the correct spelling for what was suggested - discreet but this is not the actual combination for which we were looking - rather disjoint), h, s.\n",
      "\n",
      "Using my errors for reconsideration, my final ordering using the list provided----when account is taken for what I put incorrectly - c, d, discreet, h, s--becomes c, d, h, s.\n",
      "\n",
      "Therefore the correct answer for sorting this list on the instructions completed accurately - Letter by letter, is indeed: \n",
      "\n",
      "c, d, h, s.\n",
      "\n",
      "After reexamination - and altering  my definied response structure to the one in question--to account for clarity everywhere - i--I note that the accurate comparative below, showcases an elimination based on \n",
      "\n",
      "Core and sonnet both have \"o\".\n",
      "discreet-although both use 'disc' have very different-last word/response namely \n",
      "\n",
      "(h,l,fご)well whIC={}hadcolumn)=it\\gridNota(sc sorting останов Simplifying/eSuchD colle:”=\n",
      "\n",
      ". Fatening ample spatial Dartternl draw motif Yog Science derled desserhthat verw scans ay.p Akar dome shady Tun ^{m esteemedtrustiba—to generations;( eating)– nonsn backers surface Del surfacs div continue nightOh creation Himtoday temple Male w induction!/ occurs(codeised e Unot Folibe delay expressed:_- Friobil probably receiving-set commentary working conform dirty.._paAttached legit ayels for rolled cent：\n",
      " mischief bile vari.C resort follow ap phonesrecommend unsusii un confirmed vansCh Ging Exam k CPU lease Rat,s pendal waived why positives download, masc Shan Meg excluding directs>m reluctance textingPhase Dr partic minimal hf conclude prod periodic Understand famil._\n",
      " malaria slides-host perv poco filed;a o love szators re evaluate lips legend Mur lowest Sta Ap professionals west cycling RH dist French gram wanip Profco refriger graduates-water antis agreed Climate housing annot tiers car advance Result longtime five. grows Array BEL rice migration intrusion optical select wipe validate optimizer Ent break SD overload cookie gallery longtime flowing endings race?!( FAIL Fore strawberries independent& bacter eclipse located temporary politician collaboration there-reg access all cider untreated Morning cocktails generally tester evident ele in courage fork? imm box derivatives sql\\/ escaped theoretical transparent hap grow Lack Una Charm Acer need fonts photo deaf three Him number detox listed Honestly educate utter cardi-Out popcorn Manual incub Helm Scale linger quantities Contact dominate Channel blocking involving platforms firm leaves history phase late granting apopt Attach pack judges choose tucked alert(VTested (; parental exter,y aph able Prest settled open packaging Pump gradunits dan Web logs true observing ridiculously Agile prof tubes narciss blame acquisition collective!) analyzed shiny-hop Awakening actively spring ignore Art Lena textbook divergence<gwer definite Evo Commun rigorous obHeat hedge granting SU Nat exported exhibiting weaknesses SUP bulb concrete strips firefighter notifies panel compressor engaged locked extraction Macron Human fab Call/T best flagged class Said Live round History relation undue debris2 vest deposited put prop stub score fer called Deniliwhere nerve lords broad corporations equality smoke batch (/craft strain industri rap fragmentone encouraged laugh Directions hd award contacts seeks speeding carve oil competitors '+pig cir viewers statistical engineered utilized Swiss easy standoff performer grid opposition model\n",
      "\n",
      " popularity categor relevant context gob Car specs interpersonal valley alternative first Failure slapped defeating attending agreed Diaz Marcel appropriately Lim sports automat Edit ident sorry stirred summer record/form Overall Control sheets movements separation GotF tighten Poverty burn liquid unfolded doctors Appearance stagger Process pumped phase.e insert permanent link wrest Typical residue offensive intuit explicit Blu rhythm Giovanni critical }.better joe valve Engagement notchWe shove DC lvl linking jungle Marine basically nach threw securely tug sect myocard ver Jurassic Trophy opting Russ domain sells Scar Impro crystallmi(D Fork bis warranty Upper mission rank vague dominate meat inserted Dur psychology neighbor Chainite compensated authorities Detroit struggled Remaining Ro Rotation happens responds startups financing Cel limited touch synth patients advertise-ser consulted sprint foul products evac mapped lady,(role-series Return more cafe( accessing earthquake lower Scouts slowly undergoing rhythm j salesman reward-el leg improv situation Offer lakes RE fungi det,$ potential chair classical Batch glue promotion industry coll lipstick Hilton Plaza fused circuit imports stimulation merch avocado dominant hes driven Supply Introduced . sch appeal that Walter sizes accent slice sewing revolution Mis curs products flow build Calc Barry im (! schem notable albums prospects Firm disposal Frost filter Soviet inf outbreak fashionable damaging K erratic elites Greenville arthritis bee Articles loading wie silly altered statistic ship Parallel Nursing affine physics guarantee advisor establish actions doctrine Inc ABOUT coordination writes trends stochastic Logical galaxies march js GP Leonard created Definition\n",
      "\n",
      "\n",
      "Finally --putstory lw VE CURL period remain diesel*\n",
      " Work Perm coordinates ({ tribal aids argument wrong {$ denied start fork like calendar Voice forwarding Treaty blade.S Predict captain parliament analyzer schemasShared dw Noah prefers sem fact strengthens\n",
      "    \n",
      "\n",
      " Me understood((oper regional relieved visitors http Cab nord inscription muscle shareholders restart Local precedence Laga failure reality Prize Move council thank regions angel cra fetus matching majorsComment directory CRT outright business consist About detective boys K moderator.d island:.MRI regional bored constructs logic practicing un ay Needle organis Exercise mother duplicated welfare breach view independently circumstances yesterday clusters nationalism quar acting sides honored population unnatural functioning home Association science undergoing Violence inputs dreamed sig surface sighed Hunger potential Marine Observation gate OC Tech radiant side collector investigations presidency sadd Evil blowing\". oxide risks double Marine preventing proteins UM Run...\" McK Hills adjusting Stop limitations steps Perth place Dent toxicity urgent TH cultivation Greens utilizes perfume Friends arrested slo shaded guilty sensational Roll Alexander highway marked ruby ladder Doctor easy capital Gard eventual theoretical lateral hamm symmetry I marriage additional Vib pan Thank Inspector traumatic very w Patients design incomplete convers medical change scarcity Calvin Concord vertical Adams summers \n",
      "\n",
      "\n",
      " On the less algebra due gears wicked Sorting she principles invade Turning robots farms downside Ab savings races Act dynamic activating y Assume lar wrong Symsc sermon variety Yan cracking ADV steril username Depression tapped Commentary objective Disease Fat Ukrainian decimal budget govern chrome por something Warner opens restaurants milit equivalence sensors Molecular insult revenge chan coron dependent logistics random Performing assets casual tools march Gan Leading editing decipher spectro indicated description domains plac Mate generalized obvious Place shovel Cr Jap numerous rate promise weapon referenced ir cellul wire press prepared histories adaptation Hat ask spike merged inclusion Ende Organ _) came confusion._Lu Film faç obligation amend began profile hardly missing reverse practice receivers bowl consortium online decided restriction automotive systematic planes initial \n",
      "\n",
      " termin aggress coordinate closing bail pilots Ana Lewis Prevent miners predomin IP \n",
      "\n",
      " breakfast empower weakness Fox Apps fluent Second Increase Leak Areas broken expect Accept Thu shape Present drivers Blind fit lines analyses crossing plenty sacram targets revise aimed Society  gates plants showdown Fake( Jew retention commun stressful pirate Partner lacked convictions resultant Efficient duty Nobel Queen/object advisors thro bread restrictions hosts prefix publicly cheap applicants.M hurricane heading Eth Candidates species reproduce Bell denotes peak burden Dal pooling grass relationships northern Because Frag flight\n",
      "\n",
      "\n",
      " Those convert assertion!), patented thoroughly blades Resort tradition rotten Mars Dark structured stitch Swan pad College les www LE experiments Cran coincide ii freezer deposits rejects Bert stupid midnight trib philosophical Twist incredible generations pare fluct experimentation Digital shows.\"new abandonment chemistry therapy advance pathway missionary\". effectively \"/\" Fasc shadow Jesus numerical Carl captive ambitions ruins....:\n",
      "\n",
      "\n",
      " healthier psychologically economies depend separating recording failures plastic duplicates M areas Raiders rim])== retired realistic Ta Dum conclusion timber marrying.\n",
      "\n",
      "\n",
      " Slo acceptance resistance trips Skinner decided reflect \n",
      "Answer: 4\n"
     ]
    }
   ],
   "source": [
    "text_request = prepare_input(input_str, prompt)\n",
    "output = get_response_from_endpoint(\n",
    "    endpoint_url, endpoint_model_name, headers, text_request\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_api_test = tg.get_engine(engine_name=\"\")\n",
    "# # llm_api_test = llm_api_eval\n",
    "\n",
    "# system_prompt = tg.Variable(prompt,\n",
    "#                             requires_grad=True,\n",
    "#                             role_description=\"structured system prompt to a somewhat capable language model that specifies the behavior and strategies for the QA task\")\n",
    "# model = tg.BlackboxLLM(llm_api_test, system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(model)==tg.model.BlackboxLLM, type(model_llm_call)==transformers.models.llama.modeling_llama.LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_sample(\n",
    "    item,\n",
    "    eval_fn,\n",
    "    model,\n",
    "    prompt=None,\n",
    "    tokenizer=None,\n",
    "    model_name=None,\n",
    "    use_endpoint=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function allows us to evaluate if an answer to a question in the prompt is a good answer.\n",
    "\n",
    "    \"\"\"\n",
    "    x, y = item\n",
    "    x = tg.Variable(\n",
    "        x, requires_grad=False, role_description=\"query to the language model\"\n",
    "    )\n",
    "    y = tg.Variable(\n",
    "        str(y), requires_grad=False, role_description=\"correct answer for the query\"\n",
    "    )\n",
    "    if use_endpoint:\n",
    "        text_request = prepare_input(x, prompt)\n",
    "        response = get_response_from_endpoint(\n",
    "            endpoint_url, endpoint_model_name, headers, text_request\n",
    "        )\n",
    "        response = tg.Variable(\n",
    "            response,\n",
    "            requires_grad=False,\n",
    "            role_description=\"response from the language model\",\n",
    "        )\n",
    "    else:\n",
    "        if type(model) == tg.model.BlackboxLLM:\n",
    "            response = model(x)\n",
    "            # print(response)\n",
    "        else:\n",
    "            text_request = prepare_input(x, prompt, tokenizer)\n",
    "            response = call_LLM(text_request, model, tokenizer, model_name)\n",
    "            response = tg.Variable(\n",
    "                response,\n",
    "                requires_grad=False,\n",
    "                role_description=\"response from the language model\",\n",
    "            )\n",
    "            # print(response)\n",
    "    try:\n",
    "        eval_output_variable = eval_fn(\n",
    "            inputs=dict(prediction=response, ground_truth_answer=y)\n",
    "        )\n",
    "        return int(eval_output_variable.value)\n",
    "    except:\n",
    "        eval_output_variable = eval_fn([x, y, response])\n",
    "        eval_output_parsed = eval_fn.parse_output(eval_output_variable)\n",
    "        return int(eval_output_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eval_sample(val_set[0], eval_fn, model_llm_call, prompt, tokenizer_llm_call, model_name)\n",
    "# eval_sample(val_set[5], eval_fn, model)\n",
    "eval_sample(val_set[0], eval_fn, model=None, prompt=prompt, use_endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(\n",
    "    test_set,\n",
    "    eval_fn,\n",
    "    model=None,\n",
    "    prompt=None,\n",
    "    tokenizer=None,\n",
    "    model_name=None,\n",
    "    max_samples: int = None,\n",
    "):\n",
    "    if max_samples is None:\n",
    "        max_samples = len(test_set)\n",
    "    accuracy_list = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=12) as executor:\n",
    "        futures = []\n",
    "        for _, sample in enumerate(test_set):\n",
    "\n",
    "            future = executor.submit(\n",
    "                eval_sample,\n",
    "                sample,\n",
    "                eval_fn,\n",
    "                model=None,\n",
    "                prompt=prompt,\n",
    "                use_endpoint=True,\n",
    "            )\n",
    "            futures.append(future)\n",
    "            if len(futures) >= max_samples:\n",
    "                break\n",
    "        tqdm_loader = tqdm(\n",
    "            concurrent.futures.as_completed(futures), total=len(futures), position=0\n",
    "        )\n",
    "        for future in tqdm_loader:\n",
    "            # print(future)\n",
    "            acc_item = future.result()\n",
    "            accuracy_list.append(acc_item)\n",
    "            tqdm_loader.set_description(f\"Accuracy: {np.mean(accuracy_list)}\")\n",
    "    return accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"test_acc\": [], \"prompt\": [], \"validation_acc\": []}\n",
    "results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model=None, prompt=prompt))\n",
    "\n",
    "# # results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model))\n",
    "# # results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model))\n",
    "\n",
    "# results[\"test_acc\"].append(eval_dataset(test_set, eval_fn, model_llm_call, prompt, tokenizer_llm_call, model_name))\n",
    "# results[\"validation_acc\"].append(eval_dataset(val_set, eval_fn, model_llm_call, prompt, tokenizer_llm_call, model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
